\documentclass[sts]{imsart}

\RequirePackage[OT1]{fontenc}
\usepackage{amsthm,amsmath,amssymb,natbib,xspace}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

% settings
%\pubyear{2005}
%\volume{0}
%\issue{0}
%\firstpage{1}
%\lastpage{8}

\startlocaldefs
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]

%%FJH defs
\input{FJHDef.tex}
\newcommand{\calH}{\mathcal{H}}
\newcommand{\vC}{\boldsymbol{C}}
\newcommand{\calGP}{\cg\!\cp}
\DeclareMathOperator{\Var}{\mathbb{V}}
\newcommand{\BOGOS}{\hyperlink{BriEtal18a}{BOGOS}\xspace}
\newcommand{\JH}{\hyperlink{RatHic18a}{JH}\xspace}
\endlocaldefs

\begin{document}

\begin{frontmatter}
\title{Comment on ``Probabilistic Integration: A Role in Statistical Computation?''
\thanksref{T1}}
\runtitle{Comment on ``Probabilistic Integration \ldots''}
\thankstext{T1}{This work is supported in part by NSF-DMS-1522687.}

\begin{aug}
\author{\fnms{Fred J.} \snm{Hickernell}\ead[label=e1]{hickernell@iit.edu} \ead[label=u1,url]{iit.edu/$\sim$hickernell}}
\and
\author{\fnms{R.} \snm{Jagadeeswaran}\ead[label=e2]{second@somewhere.com}}


\runauthor{Fred J. Hickernell and R. Jagadeeswaran}

\affiliation{Illinois Institute of Technology}

\address{RE 208, 10 W.\ 32$^{\text{nd}}$ St., Chicago, IL 60616
\printead{e1,e2,u1}.}

\end{aug}

\begin{abstract}
The abstract should summarize the contents of the paper.
It should be clear, descriptive, self-explanatory and not longer
than 200 words. It should also be suitable for publication in
abstracting services. Please avoid using math formulas as much as possible.

This is a sample input file.  Comparing it with the output it
generates can show you how to produce a simple document of
your own.
\end{abstract}

\begin{keyword}
\kwd{sample}
\kwd{\LaTeXe}
\end{keyword}

\end{frontmatter}

\section{When to Stop?}

In highlighting the possibilities of a probabilistic perspective on computation, the authors of \cite{BriEtal18a}, henceforth referred to as \BOGOS, have provided a useful criterion for stopping.  Numerical analysis gives us an upper bound on the approximation error expressed as a product of the roughness of the integrand and the quality of our sampling scheme.  Specifically, \BOGOS, Eq.\ (5) quotes the error bound
\begin{equation} \label{HJErrBd}
    \bigl \lvert \hat{\Pi}[f] - \Pi[f] \rvert \le \lVert f \rVert_{\calH} \lVert \mu(\hat{\pi}) - \mu(\pi) \rVert_{\calH},
\end{equation}
where the integrand, $f$, lies in a Hilbert space, $\ch$, $\Pi[f]$ denotes the integral of $f$ defined in terms of the probability measure $\pi$, and $\hat{\Pi}[f]$ denotes a cubature defined in terms of the discrete measure $\hat{\pi}$. The \emph{discrepancy} between $\pi$ and $\hat{\pi}$ is defined as $\lVert \mu(\hat{\pi}) - \mu(\pi) \rVert_{\calH}$. As the sample size, $n$, increases, one hopes that the discrepancy tends to zero.  Even if $\lVert \mu(\hat{\pi}) - \mu(\pi) \rVert_{\calH}$ can be computed efficiently, one typically does not have a good estimate or bound on $\lVert f \rVert_{\calH}$.  Therefore, it is impractical to use  \eqref{HJErrBd} to determine an $n$ that satisfies the error criterion
\begin{equation} \label{HJErrCrit}
    \bigl \lvert \hat{\Pi}[f] - \Pi[f] \bigr \rvert \le \varepsilon,
\end{equation}
where $\varepsilon$ is the absolute error tolerance.  

We believe that the practitioner would benefit from an automatic cubature, that is an algorithm with a stopping criterion that guarantees \eqref{HJErrCrit} (with high probability).  The probabilistic numerics approach espoused in \BOGOS fulfills that wish.  

Bayesian cubature assumes that the integrand, $f$, may be modeled by a Gaussian stochastic process, $g \sim \calGP(0,c)$, conditioned on $g$ having the same values as $f$ does at the cubature nodes or states, $\{\vx_i\}_{i=1}^n$.  Thus, $\hat{\Pi}[g] = \hat{\Pi}[f]$, where Bayesian cubature is chosen to satisfy $\hat{\Pi}[g] = \Ex_n[\Pi[g]]$.  Here, $c$ is the covariance function for $g$.  The definition of $g$ allows us to construct credible intervals for the cubature error via Proposition 1 in  \cite{BriEtal18a}, namely
\begin{gather}
\label{credInt}
    \Prob\Bigl[\bigl \lvert \hat{\Pi}[f] - \Pi[g] \bigr \rvert \le 2.58 \sqrt{\Var_n[\Pi[g]]}  \Bigr] = 99\%, \\
    \label{Vnform}
    \Var_n[\Pi[g]] = \Pi\Pi[c(\cdot,\cdot)]] - \Pi[\vc(\cdot,X)] \vC^{-1} \Pi[\vc(X,\cdot)].
\end{gather}
If the observed integrand, $f$, lies in the $99\%$ middle of the sample space for $g$, and not in the $1\%$ extreme, then increasing $n$ until $2.58 \sqrt{\Var_n[\Pi[g]]}$ is no greater than $\varepsilon$ ensures that  \eqref{HJErrCrit} with high probability.

There are some practical obstacles to implementing this elegant recipe in practice. 

\begin{itemize} 

\item How does one choose the covariance function $c$?  While the sample space may be chosen large enough to include $f$, our use of the credible interval as a stopping criterion assumes that our integrand is not in the tails under the assumption of the chosen covariance function.  We discuss this question in the next section.  

\item The computational cost of computing $\Var_n[\Pi[g]]$ involves matrix inversion, which requires $\mathcal{O}(n^3)$ operations in general.  This typically takes much more time than the $\mathcal{O}(n)$ operations required to compute the cubature, $\hat{\Pi}[f]$, obtaining an integrand value is quite time-consuming. We discuss how to circumvent this problem for matching kernels and cubature nodes in Section \ref{sec:Match}.

\end{itemize}

\section{Which Gaussian Process?} \label{sec:WhichGauss}
As mentioned above, using a credible as a stopping criterion requires careful choice of the covariance kernel, $c$.  The width of the credible interval in \eqref{credInt} depends on $\Var_n[\Pi[g]]$ given by \eqref{Vnform}.  For example, multiplying $c$ by four doubles the width of the credible interval.  

When constructing the confidence interval for the mean of a scalar random variable, $Y$, from independent and identically distributed (IID) data, one must estimate the variance of $Y$ by the sample variance. Analogously, when constructing the credible interval in \eqref{credInt} for the integral (mean) of a function, one must estimate the vertical scale factor inherent in $c$.

We have recently explored the feasibility of Bayesian cubature as the basis for automatically selecting $n$ to satisfy the error criterion \eqref{HJErrCrit}  in \cite{RatHic19a}, henceforth abbreviated as \JH.  We choose the covariance kernel to take the form $c(\vx, \vx') =  \lambda c_0(\vx, \vx'; \vtheta)$, where $\lambda$ is the vertical scale factor, and $\vtheta$ is a parameter determining the smoothness and other properties of the kernel.  This approach is analogous to that in \cite[Proposition 2]{BriEtal18a}.  An example is  the following \cite[(36)]{RatHic19a}:
\begin{multline}
\label{the_kernel_eqn_bernoulli}
c_0(\vx, \vx';\vtheta) =
\prod_{l=1}^d \biggl[
1 - (-1)^{r} \gamma B_{2r}( |{x_l-x'_l}| ) \biggr], \\  
\forall \vx,\vx' \in [0,1]^d, \  \vtheta = (r,\gamma), \ r \in \naturals, \ \gamma > 0,
\end{multline}
where $B_{2r}$ is the Bernoulli polynomial of degree $2r$.  The smoothness of the covariance kernel increases with $r$.  To increase the possibility that our integrand $f$ lies in the middle of the sample space, we also allow the stochastic process $g$ to have an arbitrary mean, $m$, so $g \sim \calGP(m, \lambda c_0)$.  Assuming an improper prior on $m$ and $\lambda$, the posterior marginal for $\Pi[g]$ is  a Student-t distribution with mean and variance both depending on the integrand data, $\vf = \bigl( f(\vx_i) \bigr)_{i=1}^n$ \cite[(15), (16)]{RatHic19a}:
\begin{align*}
    \hat{\Pi}[f] & =  \Ex_n[\hat{\Pi}[f]] =
\left(
\frac{  (1 - \vone^T  \vC^{-1}_0\Pi[\vc_0(X,\cdot) ] \vone^T }{ \vone^T  \vC^{-1}_0 \vone}   +  \Pi[\vc_0(\cdot, X)]
\right)  \vC^{-1}_0 \vf, \\
\Var_n[\Pi[g]] & = \frac{1}{n}
 \vf^T \left[  \vC^{-1}_0 - 
\frac{  \vC^{-1}_0 \vone \vone^T  \vC^{-1}_0 }{\vone^T  \vC^{-1}_0 \vone}
\right] \vf \\
\label{eqn:errMLE}
&\qquad \qquad \times 
\{\Pi\Pi[c_0(\cdot,\cdot)] - \Pi[\vc_0(\cdot, X)]\mC^{-1} \Pi[\vc_0(X,\cdot)] \},
\end{align*}
where $\vone$ is a vector of ones.  This is similar to the conclusion of Proposition 2 in \cite{BriEtal18a}.  

One may place a discrete prior on the parameter $\vtheta$, but this seems to us to be too arbitrary.  Thus, in \cite{RatHic19a} we advocate estimating $\vtheta$ by \emph{empirical Bayes}.  Empirical Bayes is also an alternative to the assuming improper priors on $m$ and $\lambda$.  This alternative is discussed in \cite{RatHic19a}.  For this approach the posterior marginal for $\Pi[g]$ has the same mean as for the full Bayes approach, but the variance is a bit smaller.  

We also discuss the alternative of \emph{generalized cross-validation} for estimating the correct covariance kernel from the integrand data in \cite{RatHic19a}.  The formulas for the Bayesian cubature and the credible interval width are different than for full Bayes.



\section{Speeding Up} \label{sec:Match}

\section{Bayesian Approach Versus Worst Case}


\section{Hi}


For one-dimensional integration, the standard quadrature algorithms, e.g., in MATLAB \citep{MAT9.5}, expect an input of $\varepsilon$ and adaptively determine the sample size, $n$, required.




\section*{Acknowledgements}
And this is an acknowledgements section with a heading that was produced by the
$\backslash$section* command. Thank you all for helping me writing this
\LaTeX\ sample file.

\bibliographystyle{imsart-nameyear}
\bibliography{FJH23,FJHown23}



\end{document}
